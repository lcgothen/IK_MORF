FANN_FLO_2.1
num_layers=3
learning_rate=0.100000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=1
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=4 11 4 
scale_included=1
scale_mean_in=0.097289 0.108974 -0.125845 
scale_deviation_in=0.000010 0.000011 0.000007 
scale_new_min_in=-0.900000 -0.900000 -0.900000 
scale_factor_in=0.900000 0.900000 0.900000 
scale_mean_out=-0.728676 0.855211 -1.249380 
scale_deviation_out=0.000000 0.000000 0.000130 
scale_new_min_out=-0.900000 -0.900000 -0.900000 
scale_factor_out=0.900000 0.900000 0.900000 
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 7.71227777004241943359e-01) (1, -1.02835908532142639160e-01) (2, -9.41719055175781250000e-01) (3, 1.14768736064434051514e-01) (0, 2.97067314386367797852e-01) (1, 8.21201503276824951172e-01) (2, -2.65655755996704101562e-01) (3, 1.00698351860046386719e+00) (0, -4.81110721826553344727e-01) (1, -1.83138668537139892578e-01) (2, -1.01973426342010498047e+00) (3, 6.88086628913879394531e-01) (0, -5.97587287425994873047e-01) (1, -5.17122924327850341797e-01) (2, -1.75889924168586730957e-01) (3, -1.06150574982166290283e-01) (0, 3.78148645162582397461e-01) (1, -4.11181032657623291016e-01) (2, -4.57444190979003906250e-01) (3, 3.47706407308578491211e-01) (0, -1.72593770548701286316e-03) (1, -5.78438222408294677734e-01) (2, -3.80679517984390258789e-01) (3, -6.86700418591499328613e-02) (0, 3.47737789154052734375e-01) (1, -9.48389053344726562500e-01) (2, -2.95803040266036987305e-01) (3, 1.19385468959808349609e+00) (0, -6.91466271877288818359e-01) (1, 4.01731938123703002930e-01) (2, 3.10776561498641967773e-01) (3, -1.06497085094451904297e+00) (0, -6.10545396804809570312e-01) (1, 4.60090279579162597656e-01) (2, 5.78581154346466064453e-01) (3, 8.60360682010650634766e-01) (0, 3.21764320135116577148e-01) (1, -6.46878182888031005859e-01) (2, 6.07935428619384765625e-01) (3, 8.35991084575653076172e-01) (4, 1.76939591765403747559e-01) (5, 4.82930153608322143555e-01) (6, 4.91493105888366699219e-01) (7, 7.09361076354980468750e-01) (8, -8.40615630149841308594e-01) (9, -4.57800656557083129883e-01) (10, -4.39904719591140747070e-01) (11, 7.71142423152923583984e-01) (12, -9.27715301513671875000e-02) (13, 9.62313190102577209473e-02) (14, -7.85467207431793212891e-01) (4, -8.19701194763183593750e-01) (5, 1.01766490936279296875e+00) (6, 4.51946526765823364258e-01) (7, -2.28855624794960021973e-01) (8, 1.65736362338066101074e-01) (9, 3.32166165113449096680e-01) (10, 8.43367278575897216797e-01) (11, -2.37437978386878967285e-01) (12, 8.23127150535583496094e-01) (13, 8.59490931034088134766e-02) (14, -5.83188831806182861328e-02) (4, 6.97501897811889648438e-01) (5, -4.74344240501523017883e-04) (6, 1.01849639415740966797e+00) (7, 5.26036202907562255859e-01) (8, -2.70149223506450653076e-02) (9, 1.51457414031028747559e-01) (10, 7.63306498527526855469e-01) (11, -6.44828140735626220703e-01) (12, -2.08849638700485229492e-01) (13, -2.20673754811286926270e-01) (14, -8.30217719078063964844e-01) 
