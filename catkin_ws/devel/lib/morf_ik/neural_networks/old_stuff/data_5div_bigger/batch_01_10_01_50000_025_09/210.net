FANN_FLO_2.1
num_layers=3
learning_rate=0.100000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=1
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=4 11 4 
scale_included=1
scale_mean_in=0.185956 0.050790 0.066429 
scale_deviation_in=0.008194 0.016595 0.011981 
scale_new_min_in=-0.900000 -0.900000 -0.900000 
scale_factor_in=0.900000 0.900000 0.900000 
scale_mean_out=-1.305004 1.696986 -1.863778 
scale_deviation_out=0.085569 0.175996 0.265288 
scale_new_min_out=-0.900000 -0.900000 -0.900000 
scale_factor_out=0.900000 0.900000 0.900000 
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 4.10852015018463134766e-01) (1, 1.01634454727172851562e+00) (2, 2.28774949908256530762e-01) (3, 9.34673607349395751953e-01) (0, 1.38428902626037597656e+00) (1, 6.60263419151306152344e-01) (2, 5.25570690631866455078e-01) (3, 3.04907619953155517578e-01) (0, -7.41020217537879943848e-02) (1, -8.00726890563964843750e-01) (2, -1.19971573352813720703e-01) (3, 5.10279834270477294922e-01) (0, 4.97966647148132324219e-01) (1, -9.46113586425781250000e-01) (2, -4.34531450271606445312e-01) (3, 5.14991819858551025391e-01) (0, -1.21591068804264068604e-01) (1, 9.57533478736877441406e-01) (2, -3.40005978941917419434e-02) (3, -1.81070417165756225586e-01) (0, 6.81302845478057861328e-01) (1, -1.06754827499389648438e+00) (2, 4.30387020111083984375e-01) (3, 4.79676604270935058594e-01) (0, -2.94932663440704345703e-01) (1, 7.49103486537933349609e-01) (2, -9.73988533020019531250e-01) (3, -2.91204780340194702148e-01) (0, -8.85646164417266845703e-01) (1, -6.56636059284210205078e-01) (2, -8.57143580913543701172e-01) (3, 2.59904950857162475586e-01) (0, 4.56317692995071411133e-01) (1, -4.70383375883102416992e-01) (2, 4.45256084203720092773e-01) (3, -4.10062223672866821289e-01) (0, -1.64912199974060058594e+00) (1, -7.90543735027313232422e-01) (2, -1.12401449680328369141e+00) (3, 9.26027968525886535645e-02) (4, 7.39441037178039550781e-01) (5, -5.34456409513950347900e-02) (6, -5.30029416084289550781e-01) (7, -1.17748069763183593750e+00) (8, 5.71956157684326171875e-01) (9, -1.05323648452758789062e+00) (10, 9.62688922882080078125e-01) (11, -9.35568213462829589844e-01) (12, -1.04454267024993896484e+00) (13, -2.43601799011230468750e-01) (14, 3.77298623323440551758e-01) (4, -6.92303836345672607422e-01) (5, -1.33435618877410888672e+00) (6, 2.94020861387252807617e-01) (7, -4.46983516216278076172e-01) (8, -1.32988110184669494629e-01) (9, -4.32209581136703491211e-01) (10, -7.00981438159942626953e-01) (11, 9.66193854808807373047e-01) (12, 2.92999781668186187744e-02) (13, 1.59856414794921875000e+00) (14, 4.88281846046447753906e-01) (4, -8.51491570472717285156e-01) (5, -1.19479346275329589844e+00) (6, -1.20945468544960021973e-01) (7, 7.89305090904235839844e-01) (8, 7.44771957397460937500e-02) (9, -1.48675933480262756348e-01) (10, 3.95699173212051391602e-01) (11, 6.35309696197509765625e-01) (12, -7.14184701442718505859e-01) (13, 1.87960553169250488281e+00) (14, 3.89809429645538330078e-01) 
