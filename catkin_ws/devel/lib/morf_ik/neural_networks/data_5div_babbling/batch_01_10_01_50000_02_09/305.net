FANN_FLO_2.1
num_layers=3
learning_rate=0.100000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=1
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=4 11 4 
scale_included=1
scale_mean_in=0.163901 -0.045124 0.007243 
scale_deviation_in=0.007881 0.009595 0.010414 
scale_new_min_in=-0.900000 -0.900000 -0.900000 
scale_factor_in=0.900000 0.900000 0.900000 
scale_mean_out=-1.839069 2.072781 -0.925718 
scale_deviation_out=0.055199 0.102323 0.140100 
scale_new_min_out=-0.900000 -0.900000 -0.900000 
scale_factor_out=0.900000 0.900000 0.900000 
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 6.63742244243621826172e-01) (1, -7.04643487930297851562e-01) (2, -1.20535111427307128906e+00) (3, 4.80194747447967529297e-01) (0, 1.08233904838562011719e+00) (1, 2.77599483728408813477e-01) (2, -1.48795977234840393066e-01) (3, 5.93312680721282958984e-01) (0, 2.05748192965984344482e-02) (1, -2.58433669805526733398e-01) (2, -2.86290884017944335938e-01) (3, 4.07697081565856933594e-01) (0, 6.03025734424591064453e-01) (1, -1.61605632305145263672e+00) (2, -7.16950058937072753906e-01) (3, -2.52951353788375854492e-01) (0, -1.07517652213573455811e-01) (1, 1.28852140903472900391e+00) (2, -5.14401316642761230469e-01) (3, -2.50229164958000183105e-02) (0, 7.49422848224639892578e-01) (1, 5.51875591278076171875e-01) (2, -7.53422826528549194336e-02) (3, -6.83426201343536376953e-01) (0, -1.13130807876586914062e+00) (1, 7.83250391483306884766e-01) (2, 7.46835693717002868652e-02) (3, -3.16467046737670898438e-01) (0, -1.04437458515167236328e+00) (1, 1.56843021512031555176e-01) (2, -1.19472004473209381104e-01) (3, 8.63948941230773925781e-01) (0, -8.32728803157806396484e-01) (1, -5.60741961002349853516e-01) (2, 7.68245160579681396484e-01) (3, 1.21195890009403228760e-01) (0, -1.81414461135864257812e+00) (1, -4.19979095458984375000e-02) (2, -2.08667173981666564941e-01) (3, -4.49128672480583190918e-02) (4, -4.85021561384201049805e-01) (5, 7.11043953895568847656e-01) (6, -2.98626124858856201172e-01) (7, -1.43770813941955566406e+00) (8, 1.37020790576934814453e+00) (9, 6.65598928928375244141e-01) (10, 3.52138042449951171875e-01) (11, -5.56996226310729980469e-01) (12, -9.98948991298675537109e-01) (13, -1.40374079346656799316e-01) (14, 2.88589358329772949219e-01) (4, -1.41036617755889892578e+00) (5, -8.29371511936187744141e-01) (6, 5.46828031539916992188e-01) (7, -1.05789434909820556641e+00) (8, 2.04849690198898315430e-02) (9, 4.76920366287231445312e-01) (10, 2.21179530024528503418e-01) (11, 6.42994403839111328125e-01) (12, 1.19470787048339843750e+00) (13, 1.24322271347045898438e+00) (14, 2.01480105519294738770e-01) (4, 5.71690857410430908203e-01) (5, -1.09826886653900146484e+00) (6, -3.72898906469345092773e-01) (7, -2.53354072570800781250e-01) (8, 9.85900282859802246094e-01) (9, -5.52672326564788818359e-01) (10, 1.03666424751281738281e+00) (11, 3.18246670067310333252e-02) (12, -5.97893297672271728516e-01) (13, 1.72203242778778076172e+00) (14, 3.04385095834732055664e-01) 
