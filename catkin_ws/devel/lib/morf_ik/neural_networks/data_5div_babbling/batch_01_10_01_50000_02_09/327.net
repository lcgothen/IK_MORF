FANN_FLO_2.1
num_layers=3
learning_rate=0.100000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=1
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=4 11 4 
scale_included=1
scale_mean_in=0.166220 0.050039 0.083572 
scale_deviation_in=0.007051 0.014455 0.010715 
scale_new_min_in=-0.900000 -0.900000 -0.900000 
scale_factor_in=0.900000 0.900000 0.900000 
scale_mean_out=-1.279160 2.031552 -1.638799 
scale_deviation_out=0.083411 0.206324 0.329327 
scale_new_min_out=-0.900000 -0.900000 -0.900000 
scale_factor_out=0.900000 0.900000 0.900000 
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 2.40020332336425781250e+01) (1, 1.54234113693237304688e+01) (2, 1.70240364074707031250e+01) (3, 3.24079246520996093750e+01) (0, 5.47371177673339843750e+01) (1, 4.02286872863769531250e+01) (2, 4.72584495544433593750e+01) (3, -3.67971076965332031250e+01) (0, -2.16885318756103515625e+01) (1, -1.31186161041259765625e+01) (2, -1.57835388183593750000e+01) (3, -3.00581226348876953125e+01) (0, 4.60791397094726562500e+01) (1, 3.56887397766113281250e+01) (2, 4.10514907836914062500e+01) (3, -3.43820915222167968750e+01) (0, -2.38252239227294921875e+01) (1, -1.67402057647705078125e+01) (2, -1.76446762084960937500e+01) (3, -3.26785354614257812500e+01) (0, 2.01398639678955078125e+01) (1, 1.58776636123657226562e+01) (2, 1.46253757476806640625e+01) (3, 2.88709526062011718750e+01) (0, -1.75676155090332031250e+01) (1, -4.68086128234863281250e+01) (2, -1.17354774475097656250e+01) (3, -3.15730547904968261719e+00) (0, -1.00612792968750000000e+01) (1, -3.20946578979492187500e+01) (2, -3.48210487365722656250e+01) (3, 1.22863225936889648438e+01) (0, 3.59146690368652343750e+01) (1, -1.04720563888549804688e+01) (2, 2.37157344818115234375e+01) (3, -3.45308685302734375000e+00) (0, 5.16487655639648437500e+01) (1, 9.21002674102783203125e+00) (2, 8.06222629547119140625e+00) (3, 1.92251892089843750000e+01) (4, -1.59183824062347412109e+00) (5, -1.22269049286842346191e-01) (6, 2.38835763931274414062e+00) (7, 4.55899119377136230469e-01) (8, -4.61993366479873657227e-01) (9, 3.91075134277343750000e+00) (10, -1.67189359664916992188e+00) (11, -3.67173522710800170898e-01) (12, -1.14007103443145751953e+00) (13, -5.27708530426025390625e-01) (14, -1.12973578274250030518e-01) (4, -2.86077899932861328125e+01) (5, -7.16670608520507812500e+01) (6, 2.58776702880859375000e+01) (7, -6.66850357055664062500e+01) (8, 2.64335823059082031250e+01) (9, -2.47832927703857421875e+01) (10, 3.86177749633789062500e+01) (11, 4.09403915405273437500e+01) (12, -3.85229148864746093750e+01) (13, -4.13108062744140625000e+01) (14, -3.21168327331542968750e+01) (4, -2.64903316497802734375e+01) (5, -7.18475570678710937500e+01) (6, 2.70124416351318359375e+01) (7, -6.74080657958984375000e+01) (8, 2.83890533447265625000e+01) (9, -2.61694736480712890625e+01) (10, 3.69768371582031250000e+01) (11, 4.02930068969726562500e+01) (12, -3.73082809448242187500e+01) (13, -3.99054069519042968750e+01) (14, -3.09737548828125000000e+01) 
