FANN_FLO_2.1
num_layers=3
learning_rate=0.100000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=1
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=4 11 4 
scale_included=1
scale_mean_in=0.162312 0.048498 0.006876 
scale_deviation_in=0.008922 0.014520 0.010365 
scale_new_min_in=-0.900000 -0.900000 -0.900000 
scale_factor_in=0.900000 0.900000 0.900000 
scale_mean_out=-1.280632 2.073713 -0.921641 
scale_deviation_out=0.087723 0.107297 0.144668 
scale_new_min_out=-0.900000 -0.900000 -0.900000 
scale_factor_out=0.900000 0.900000 0.900000 
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 3.33120018243789672852e-01) (1, -9.33306396007537841797e-01) (2, 6.99295043945312500000e-01) (3, 1.89628824591636657715e-01) (0, -1.74782657623291015625e+00) (1, 4.74336981773376464844e-01) (2, 9.50959473848342895508e-02) (3, 2.64137536287307739258e-01) (0, -1.00222527980804443359e+00) (1, -8.11572849750518798828e-01) (2, -2.53015935420989990234e-01) (3, -5.11752724647521972656e-01) (0, -1.61999344825744628906e+00) (1, -5.60096800327301025391e-01) (2, 1.20972859859466552734e+00) (3, -2.58836120367050170898e-01) (0, 1.97809800505638122559e-01) (1, -1.14768457412719726562e+00) (2, 3.62088263034820556641e-01) (3, -2.01482176780700683594e-01) (0, -7.11917400360107421875e-01) (1, -1.25872159004211425781e+00) (2, -6.26700520515441894531e-01) (3, 1.79263100028038024902e-01) (0, 4.71434235572814941406e-01) (1, -1.21006286144256591797e+00) (2, -2.36027911305427551270e-01) (3, 5.04628539085388183594e-01) (0, 3.33525121212005615234e-01) (1, -1.36463671922683715820e-01) (2, 3.26225638389587402344e-01) (3, -1.74177989363670349121e-01) (0, 8.63538444042205810547e-01) (1, 4.93401259183883666992e-01) (2, 1.18711495399475097656e+00) (3, 5.95011889934539794922e-01) (0, 3.27509731054306030273e-01) (1, 5.52761912345886230469e-01) (2, -3.69140028953552246094e-01) (3, -3.31694483757019042969e-01) (4, -1.01753401756286621094e+00) (5, 1.22365832328796386719e+00) (6, -3.49693745374679565430e-01) (7, -1.39960318803787231445e-01) (8, -1.16143333911895751953e+00) (9, -8.69378149509429931641e-01) (10, -1.29063701629638671875e+00) (11, -3.83174955844879150391e-01) (12, 7.35688209533691406250e-01) (13, 8.40059697628021240234e-01) (14, 7.27622359991073608398e-02) (4, 3.63218069076538085938e-01) (5, 1.69051480293273925781e+00) (6, 6.62696599960327148438e-01) (7, 2.10193347930908203125e+00) (8, 7.08587765693664550781e-01) (9, 6.70550048351287841797e-01) (10, -6.34667575359344482422e-02) (11, 7.47792422771453857422e-02) (12, 1.98333129286766052246e-01) (13, -1.01059901714324951172e+00) (14, 2.39941049367189407349e-02) (4, -1.61217302083969116211e-01) (5, 1.87229251861572265625e+00) (6, 7.82092154026031494141e-01) (7, 6.08011722564697265625e-01) (8, 1.62601351737976074219e-01) (9, 1.31727325916290283203e+00) (10, 5.43823063373565673828e-01) (11, -3.83383393287658691406e-01) (12, -1.08456850051879882812e+00) (13, -3.22725236415863037109e-01) (14, 1.27842873334884643555e-01) 
