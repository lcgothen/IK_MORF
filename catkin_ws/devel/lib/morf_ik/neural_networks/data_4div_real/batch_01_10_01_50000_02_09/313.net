FANN_FLO_2.1
num_layers=3
learning_rate=0.100000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=1
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=4 11 4 
scale_included=1
scale_mean_in=0.226085 0.016868 -0.032113 
scale_deviation_in=0.018065 0.019259 0.012037 
scale_new_min_in=-0.900000 -0.900000 -0.900000 
scale_factor_in=0.900000 0.900000 0.900000 
scale_mean_out=-1.494197 1.348668 -1.789460 
scale_deviation_out=0.087645 0.456546 0.925697 
scale_new_min_out=-0.900000 -0.900000 -0.900000 
scale_factor_out=0.900000 0.900000 0.900000 
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -5.47767698764801025391e-01) (1, 6.01474821567535400391e-01) (2, -6.76856160163879394531e-01) (3, 8.27035367488861083984e-01) (0, 2.15159028768539428711e-01) (1, 2.84706801176071166992e-01) (2, 2.03312009572982788086e-01) (3, 1.82060003280639648438e-01) (0, -1.73136964440345764160e-02) (1, -3.54829192161560058594e+00) (2, 1.20077384635806083679e-02) (3, -3.85560798645019531250e+00) (0, -3.34478735923767089844e-01) (1, 1.63791418075561523438e-01) (2, 4.33239489793777465820e-01) (3, 5.55136442184448242188e-01) (0, -4.14678633213043212891e-01) (1, -4.14707660675048828125e-01) (2, 1.55927836894989013672e-01) (3, -4.34574514627456665039e-01) (0, -2.20240466296672821045e-02) (1, 7.46632665395736694336e-02) (2, 1.71038663387298583984e+00) (3, 1.48880973458290100098e-01) (0, 1.12762546539306640625e+00) (1, 1.72826319932937622070e-01) (2, 2.71243322640657424927e-02) (3, 1.02989661693572998047e+00) (0, -7.03267097473144531250e-01) (1, -3.64900231361389160156e-01) (2, -3.21976989507675170898e-01) (3, -5.17060101032257080078e-01) (0, -4.02720499038696289062e+00) (1, -4.49726194143295288086e-01) (2, -3.42442601919174194336e-01) (3, 2.90627169609069824219e+00) (0, -5.41134953498840332031e-01) (1, 3.29205060005187988281e+00) (2, -6.19277432560920715332e-02) (3, -3.74007487297058105469e+00) (4, 6.35683298110961914062e-01) (5, 1.58281207084655761719e+00) (6, -3.80816888809204101562e+00) (7, 6.15913033485412597656e-01) (8, -9.16547179222106933594e-01) (9, -1.20418205857276916504e-01) (10, -6.89165055751800537109e-01) (11, -5.67200303077697753906e-01) (12, -6.06734585016965866089e-03) (13, 4.04613971710205078125e+00) (14, -3.59907478094100952148e-01) (4, 9.31606113910675048828e-01) (5, -1.74584996700286865234e+00) (6, -5.82386314868927001953e-01) (7, 1.40180861949920654297e+00) (8, 1.44793832302093505859e+00) (9, 8.45797657966613769531e-01) (10, -1.24591243267059326172e+00) (11, 9.86758708953857421875e-01) (12, 2.18829584121704101562e+00) (13, 7.03452751040458679199e-02) (14, -8.42023074626922607422e-01) (4, 1.07827268540859222412e-01) (5, 5.63125371932983398438e-01) (6, -8.62986221909523010254e-02) (7, 9.76414680480957031250e-01) (8, 5.87023794651031494141e-01) (9, -3.66351246833801269531e-01) (10, -1.24926316738128662109e+00) (11, 1.59354254603385925293e-01) (12, 3.98987472057342529297e-01) (13, -8.39287340641021728516e-02) (14, 7.70218372344970703125e-02) 
