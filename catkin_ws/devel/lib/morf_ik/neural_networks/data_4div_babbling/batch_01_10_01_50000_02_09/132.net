FANN_FLO_2.1
num_layers=3
learning_rate=0.100000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=1
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=4 11 4 
scale_included=1
scale_mean_in=0.097302 0.108986 -0.054939 
scale_deviation_in=0.000000 0.000000 0.000000 
scale_new_min_in=-0.900000 -0.900000 -0.900000 
scale_factor_in=0.900000 0.900000 0.900000 
scale_mean_out=-0.728676 1.797690 -0.615314 
scale_deviation_out=0.000000 0.000000 0.000000 
scale_new_min_out=-0.900000 -0.900000 -0.900000 
scale_factor_out=0.900000 0.900000 0.900000 
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 6.94255924224853515625e+00) (1, 7.67033195495605468750e+00) (2, -4.84145545959472656250e+00) (3, 7.09159317016601562500e+01) (0, -6.99863719940185546875e+00) (1, -7.13553142547607421875e+00) (2, 3.11516094207763671875e+00) (3, -7.08230514526367187500e+01) (0, -7.04364776611328125000e+00) (1, -7.02113914489746093750e+00) (2, 4.06067323684692382812e+00) (3, -7.25256423950195312500e+01) (0, -7.26649093627929687500e+00) (1, -8.35229206085205078125e+00) (2, 4.42635917663574218750e+00) (3, -7.12946319580078125000e+01) (0, -5.83729076385498046875e+00) (1, -6.27412796020507812500e+00) (2, 2.75184631347656250000e+00) (3, -6.58108749389648437500e+01) (0, -7.51546573638916015625e+00) (1, -7.57225418090820312500e+00) (2, 3.68779182434082031250e+00) (3, -6.89303283691406250000e+01) (0, 5.95316123962402343750e+00) (1, 6.01917457580566406250e+00) (2, -4.39245128631591796875e+00) (3, 6.50199584960937500000e+01) (0, -6.25537395477294921875e+00) (1, -6.91691493988037109375e+00) (2, 3.63544273376464843750e+00) (3, -6.84287490844726562500e+01) (0, 6.39963054656982421875e+00) (1, 7.65708303451538085938e+00) (2, -4.77711820602416992188e+00) (3, 7.12911834716796875000e+01) (0, 6.36868619918823242188e+00) (1, 7.12518835067749023438e+00) (2, -3.10451197624206542969e+00) (3, 7.14929885864257812500e+01) (4, -1.08318924903869628906e+00) (5, 2.97040026634931564331e-02) (6, 7.62736499309539794922e-01) (7, -6.76702558994293212891e-02) (8, 3.56955736875534057617e-01) (9, -4.61028516292572021484e-02) (10, 1.45322024822235107422e-01) (11, -7.03821778297424316406e-02) (12, -1.92681804299354553223e-01) (13, 4.81126219034194946289e-01) (14, -2.37133830785751342773e-01) (4, 8.24766769409179687500e+01) (5, -8.23378524780273437500e+01) (6, -8.33731536865234375000e+01) (7, -8.27569885253906250000e+01) (8, -7.91418228149414062500e+01) (9, -8.12790832519531250000e+01) (10, 7.86928329467773437500e+01) (11, -8.08512496948242187500e+01) (12, 8.26738128662109375000e+01) (13, 8.27059555053710937500e+01) (14, 8.52557601928710937500e+01) (4, -7.69246935844421386719e-01) (5, -6.59901201725006103516e-01) (6, -2.75688529014587402344e-01) (7, -8.42318236827850341797e-01) (8, 7.22013294696807861328e-01) (9, 1.21239498257637023926e-01) (10, -3.28941166400909423828e-01) (11, 5.02291858196258544922e-01) (12, -2.37908318638801574707e-01) (13, -5.75669109821319580078e-02) (14, -4.73557472229003906250e-01) 
