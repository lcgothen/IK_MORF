FANN_FLO_2.1
num_layers=3
learning_rate=0.100000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=1
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=4 11 4 
scale_included=1
scale_mean_in=0.077231 -0.022377 -0.169197 
scale_deviation_in=0.013262 0.004725 0.013483 
scale_new_min_in=-0.900000 -0.900000 -0.900000 
scale_factor_in=0.900000 0.900000 0.900000 
scale_mean_out=-1.855335 0.247296 -1.393368 
scale_deviation_out=0.049109 0.210902 0.282406 
scale_new_min_out=-0.900000 -0.900000 -0.900000 
scale_factor_out=0.900000 0.900000 0.900000 
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 5.32584249973297119141e-01) (1, 6.74904882907867431641e-02) (2, 1.56029433012008666992e-01) (3, 7.17245221138000488281e-01) (0, -6.17789864540100097656e-01) (1, -3.65011483430862426758e-01) (2, 6.25487208366394042969e-01) (3, -2.39308610558509826660e-01) (0, -1.29637908935546875000e+00) (1, -1.28647422790527343750e+00) (2, 2.46717661619186401367e-01) (3, 1.63653194904327392578e-01) (0, -7.42140352725982666016e-01) (1, -1.02305126190185546875e+00) (2, -1.41890788078308105469e+00) (3, 1.66579261422157287598e-01) (0, 1.15375268459320068359e+00) (1, -3.09520602226257324219e-01) (2, -1.19662351906299591064e-01) (3, 7.55757629871368408203e-01) (0, 1.14693112671375274658e-01) (1, -1.04191589355468750000e+00) (2, -5.58449506759643554688e-01) (3, 7.64784395694732666016e-01) (0, 1.54555082321166992188e-01) (1, 8.95034074783325195312e-02) (2, 1.42478966712951660156e+00) (3, -3.79610389471054077148e-01) (0, 7.49198138713836669922e-01) (1, -3.86418163776397705078e-01) (2, -4.77127373218536376953e-01) (3, 1.97211429476737976074e-01) (0, 3.50848466157913208008e-01) (1, 9.12754774093627929688e-01) (2, -5.43455243110656738281e-01) (3, 8.38245511054992675781e-01) (0, -3.53247404098510742188e-01) (1, 6.85236692428588867188e-01) (2, -9.34707283973693847656e-01) (3, -2.91724018752574920654e-02) (4, 2.41807788610458374023e-01) (5, -4.70752984285354614258e-01) (6, -2.16919445991516113281e+00) (7, -9.39678966999053955078e-01) (8, 7.59429514408111572266e-01) (9, -1.04648315906524658203e+00) (10, -4.46218699216842651367e-02) (11, 1.42065197229385375977e-01) (12, 5.54752647876739501953e-01) (13, 5.47949671745300292969e-01) (14, 1.37516409158706665039e-02) (4, -4.74350810050964355469e-01) (5, 7.15014874935150146484e-01) (6, 3.68196338415145874023e-01) (7, -1.70495438575744628906e+00) (8, 8.54551196098327636719e-02) (9, -2.02438443899154663086e-01) (10, 1.52898609638214111328e+00) (11, 2.67093479633331298828e-01) (12, -7.36721396446228027344e-01) (13, -1.09249687194824218750e+00) (14, 9.27291035652160644531e-01) (4, 2.11385250091552734375e-01) (5, 1.03724825382232666016e+00) (6, 1.09597373008728027344e+00) (7, -1.32955992221832275391e+00) (8, 3.17560940980911254883e-01) (9, -1.01388502120971679688e+00) (10, 1.17101502418518066406e+00) (11, -1.01021039485931396484e+00) (12, 4.30884622037410736084e-02) (13, -7.19157397747039794922e-01) (14, 6.41903281211853027344e-01) 
