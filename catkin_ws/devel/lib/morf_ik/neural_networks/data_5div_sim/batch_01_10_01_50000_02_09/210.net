FANN_FLO_2.1
num_layers=3
learning_rate=0.100000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=1
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=4 11 4 
scale_included=1
scale_mean_in=0.096091 -0.004303 -0.173476 
scale_deviation_in=0.015489 0.014352 0.010501 
scale_new_min_in=-0.900000 -0.900000 -0.900000 
scale_factor_in=0.900000 0.900000 0.900000 
scale_mean_out=-1.615913 0.197506 -1.597849 
scale_deviation_out=0.152650 0.181582 0.266652 
scale_new_min_out=-0.900000 -0.900000 -0.900000 
scale_factor_out=0.900000 0.900000 0.900000 
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (11, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 7.92441964149475097656e-01) (1, 1.03580522537231445312e+00) (2, -9.71237123012542724609e-01) (3, -8.26498791575431823730e-02) (0, -1.59466907382011413574e-01) (1, -1.24246847629547119141e+00) (2, 5.83650231361389160156e-01) (3, -6.53892815113067626953e-01) (0, 2.05177128314971923828e-01) (1, -3.39066892862319946289e-01) (2, -1.25731587409973144531e+00) (3, -2.88734376430511474609e-01) (0, -6.20337545871734619141e-01) (1, -1.02924227714538574219e-01) (2, -3.88533413410186767578e-01) (3, -1.82973831892013549805e-01) (0, 2.03121230006217956543e-01) (1, -4.92257416248321533203e-01) (2, 1.21191370487213134766e+00) (3, -6.94111764430999755859e-01) (0, 4.06168252229690551758e-01) (1, 7.44964927434921264648e-02) (2, 9.65406239032745361328e-01) (3, 1.16043888032436370850e-01) (0, 9.96185839176177978516e-01) (1, -2.97479536384344100952e-02) (2, -7.67992198467254638672e-01) (3, -6.90736353397369384766e-01) (0, 5.43414294719696044922e-01) (1, -1.17812931537628173828e+00) (2, -2.81456112861633300781e-01) (3, 3.45164448022842407227e-01) (0, -8.74090731143951416016e-01) (1, 8.21717157959938049316e-02) (2, 1.52283620834350585938e+00) (3, 3.80827873945236206055e-01) (0, -6.78216442465782165527e-02) (1, 1.59123790264129638672e+00) (2, -5.74957728385925292969e-01) (3, 5.25105178356170654297e-01) (4, 1.00246036052703857422e+00) (5, -1.14421272277832031250e+00) (6, -4.80755269527435302734e-01) (7, -9.24858599901199340820e-02) (8, 1.74132809042930603027e-01) (9, 5.58396995067596435547e-01) (10, -2.80193507671356201172e-01) (11, -9.24582123756408691406e-01) (12, 2.27550908923149108887e-01) (13, 1.30885982513427734375e+00) (14, -5.31867802143096923828e-01) (4, -2.83897757530212402344e-01) (5, -6.99872791767120361328e-01) (6, -1.41033470630645751953e+00) (7, -6.35918259620666503906e-01) (8, 1.18770730495452880859e+00) (9, 1.04470324516296386719e+00) (10, -3.54477912187576293945e-01) (11, 6.05154097080230712891e-01) (12, 1.13004922866821289062e+00) (13, 5.43530285358428955078e-02) (14, -4.08522486686706542969e-01) (4, -7.64127731323242187500e-01) (5, 6.42999827861785888672e-01) (6, -6.21914923191070556641e-01) (7, -1.53416574001312255859e-01) (8, 4.36905831098556518555e-01) (9, 4.74702090024948120117e-01) (10, -9.24832642078399658203e-01) (11, -6.13668084144592285156e-01) (12, 1.15240466594696044922e+00) (13, 3.95218968391418457031e-01) (14, -1.30618333816528320312e-01) 
