FANN_FLO_2.1
num_layers=4
learning_rate=0.100000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=1
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535522461e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000005960464477539e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=4 6 6 4 
scale_included=1
scale_mean_in=0.058524 0.109824 0.138295 
scale_deviation_in=0.016651 0.011992 0.006277 
scale_new_min_in=-0.900000 -0.900000 -0.900000 
scale_factor_in=0.900000 0.900000 0.900000 
scale_mean_out=-0.487409 2.210244 -1.952538 
scale_deviation_out=0.132313 0.184373 0.238836 
scale_new_min_out=-0.900000 -0.900000 -0.900000 
scale_factor_out=0.900000 0.900000 0.900000 
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (4, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (6, 5, 5.00000000000000000000e-01) (6, 5, 5.00000000000000000000e-01) (6, 5, 5.00000000000000000000e-01) (6, 5, 5.00000000000000000000e-01) (6, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) (6, 5, 5.00000000000000000000e-01) (6, 5, 5.00000000000000000000e-01) (6, 5, 5.00000000000000000000e-01) (0, 5, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, -6.28020286560058593750e-01) (1, -6.80281758308410644531e-01) (2, -5.36555886268615722656e-01) (3, -1.00704300403594970703e+00) (0, 2.48486712574958801270e-01) (1, 4.46163415908813476562e-01) (2, -7.31639325618743896484e-01) (3, -2.17674896121025085449e-02) (0, 1.21570205688476562500e+00) (1, 1.34750175476074218750e+00) (2, 8.88044178485870361328e-01) (3, -1.83843171596527099609e+00) (0, 6.97063386440277099609e-01) (1, -4.66271132230758666992e-01) (2, 3.86240035295486450195e-01) (3, -1.03882896900177001953e+00) (0, -7.00643897056579589844e-01) (1, 4.52631384134292602539e-01) (2, 4.51756775379180908203e-01) (3, -9.04038429260253906250e-01) (4, -1.86624372005462646484e+00) (5, -1.20576582849025726318e-01) (6, 8.12309920787811279297e-01) (7, -6.62437498569488525391e-01) (8, 1.52866208553314208984e+00) (9, 8.64378392696380615234e-01) (4, -1.40199983119964599609e+00) (5, 1.08798050880432128906e+00) (6, 8.32609534263610839844e-01) (7, 1.32439529895782470703e+00) (8, -2.53436304628849029541e-02) (9, 6.96390926837921142578e-01) (4, -1.12528002262115478516e+00) (5, -1.06488788127899169922e+00) (6, 4.81085062026977539062e-01) (7, 1.29116332530975341797e+00) (8, 2.54932910203933715820e-01) (9, 6.80317878723144531250e-01) (4, -3.03188890218734741211e-01) (5, -3.65137420594692230225e-02) (6, 1.46476283669471740723e-01) (7, 8.82504463195800781250e-01) (8, -1.75786745548248291016e+00) (9, -5.02539098262786865234e-01) (4, -9.05563175678253173828e-01) (5, 4.27280336618423461914e-01) (6, 2.41542601585388183594e+00) (7, 1.60258972644805908203e+00) (8, 2.64181518554687500000e+00) (9, 8.47388803958892822266e-01) (10, 1.94858193397521972656e+00) (11, -2.19016647338867187500e+00) (12, -1.62814652919769287109e+00) (13, -3.57649540901184082031e+00) (14, 8.58373701572418212891e-01) (15, -1.09807960689067840576e-01) (10, -2.27779436111450195312e+00) (11, -3.29789400100708007812e+00) (12, 2.78729107230901718140e-03) (13, 1.90775375813245773315e-02) (14, -3.12456631660461425781e+00) (15, -7.98204600811004638672e-01) (10, -2.39907670021057128906e+00) (11, -1.90417432785034179688e+00) (12, -2.60229730606079101562e+00) (13, 4.34582889080047607422e-01) (14, -2.95898175239562988281e+00) (15, -3.31541776657104492188e-01) 
